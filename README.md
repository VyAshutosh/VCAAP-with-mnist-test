# VCAAP-with-mnist-test
VCCA a deep multi-view learning model that extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by deep neural networks. We derive variational lower bounds of the data likelihood by parameterizing the posterior probability of the latent variables from the view that is available at test time. We also propose a variant of VCCA called VCCA-private that can,in addition to the “common variables” underlying both views, extract the “private variables”within each view, and disentangles the sharedand private information for multi-view data with-out hard supervision.  Experimental results onreal-world datasets show that our methods arecompetitive across domains.
We first demonstrate our algorithms on the noisy MNIST dataset used byWang et al.(2015b). The dataset is generated using the MNIST dataset (LeCun et al.,1998), which consists of 28×28 grayscale digit images, with 60K/10K images for training/testing.  We first linearly rescale thepixel values to the range [0,1]. Then, we randomly rotate the images at angles uniformly sampled from [−π/4, π/4] and the resulting images are used as view 1 inputs. Foreach view 1 image, we randomly select an image of the same identity (0-9) from the original dataset, add independent random noise uniformly sampled from [0,1] to eachpixel, and truncate the pixel final values to [0,1] to obtainthe corresponding view 2 sample.The original training setis further split into training/tuning sets of size 50K/10K. The data generation process ensures that the digit identity is the only common variable underlying both views.
The effect of private variables on reconstructions shows sample reconstructions (mean and standard deviation) by VCCA for the view 2 images from the test set. We observe that for each input, the mean reconstruction of yi by VCCA is a prototypical image of the same digit, regardless of the individual style in yi. This is to be expected, as yi contains an arbitrary image of the same digit as xi, and the variation in background noise in yi does not appear in xi and can not be reflected in qφ(z|x); thus the best way for pθ(y|z) to model yi is to output a prototypical image of that class to achieve on average small reconstruction error. On the other hand, since yi contains little rotation of the digits, this variation is suppressed to a large extent in qφ(z|x).
With the help of private variables hy(as part of the input to pθ(y|z,hy)), the model does a much better job in reconstructing the styles of y. And by disentangling the private variables from the shared variables,qφ(z|x) achieves even better class separation than VCCA does. We also note that the standard deviation of the reconstruction is low within the digit and high outside the digit, implying that pθ(y|z,hy) is able to separate the background noise from the digit image.
